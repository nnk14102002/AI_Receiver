{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d31188",
   "metadata": {
    "id": "69d31188"
   },
   "source": [
    "## GPU Configuration and Imports <a class=\"anchor\" id=\"GPU-Configuration-and-Imports\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6393b2fe",
   "metadata": {
    "executionInfo": {
     "elapsed": 4267,
     "status": "ok",
     "timestamp": 1742286319271,
     "user": {
      "displayName": "Nam Khánh",
      "userId": "17195362204309494329"
     },
     "user_tz": -420
    },
    "id": "6393b2fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-19 06:45:22.590718: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747637122.731207    2213 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747637122.768085    2213 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747637123.050624    2213 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747637123.050661    2213 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747637123.050664    2213 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747637123.050667    2213 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-19 06:45:23.079002: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-19 06:45:29.023089: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "# Avoid warnings from TensorFlow\n",
    "tf.get_logger().setLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "zEoWxhRlzS4-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1742286319289,
     "user": {
      "displayName": "Nam Khánh",
      "userId": "17195362204309494329"
     },
     "user_tz": -420
    },
    "id": "zEoWxhRlzS4-",
    "outputId": "ebfae43b-7d83-40d5-b239-0a5d54bb285b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6244a108",
   "metadata": {
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1742286365494,
     "user": {
      "displayName": "Nam Khánh",
      "userId": "17195362204309494329"
     },
     "user_tz": -420
    },
    "id": "6244a108"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import h5py\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer, Conv2D, LayerNormalization, SeparableConv2D, Normalization, BatchNormalization, Dense\n",
    "from tensorflow.nn import relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33937d",
   "metadata": {
    "id": "6d33937d"
   },
   "source": [
    "## **Neural Receiver <a class=\"anchor\" id=\"Neural-Receiver\"></a>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "mzrAkhF2067G",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1742286366909,
     "user": {
      "displayName": "Nam Khánh",
      "userId": "17195362204309494329"
     },
     "user_tz": -420
    },
    "id": "mzrAkhF2067G"
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(Model):\n",
    "    def __init__(self):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # self._layer_norm_1 = LayerNormalization(axis=[-1])\n",
    "        self._conv_1 = SeparableConv2D(filters= 64,\n",
    "                              kernel_size=[3,3],\n",
    "                              padding='same',\n",
    "                              activation=None)\n",
    "\n",
    "        # self._layer_norm_2 = LayerNormalization(axis=[-1])\n",
    "        self._conv_2 = SeparableConv2D(filters= 64,\n",
    "                              kernel_size=[3,3],\n",
    "                              padding='same',\n",
    "                              activation=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # z = self._layer_norm_1(inputs)\n",
    "        z = relu(inputs)\n",
    "        z = self._conv_1(z)\n",
    "        # z = self._layer_norm_2(z)\n",
    "        z = relu(z)\n",
    "        z = self._conv_2(z) # [batch size, num time samples, num subcarriers, num_channels]\n",
    "        # Skip connection\n",
    "        z = z + inputs\n",
    "\n",
    "        return z\n",
    "\n",
    "class CustomNeuralReceiver(Model):\n",
    "    def __init__(self, num_res_block):\n",
    "        super(CustomNeuralReceiver, self).__init__()\n",
    "        self._num_resblock = num_res_block\n",
    "\n",
    "        # Input convolution\n",
    "        self._input_conv = SeparableConv2D(filters= 64,\n",
    "                                  kernel_size=[3,3],\n",
    "                                  padding='same',\n",
    "                                  activation=None)\n",
    "        \n",
    "    \n",
    "        # Residual blocks\n",
    "        self._res_block = [ResidualBlock() for i in range(self._num_resblock)]\n",
    "        # self._res_block_1 = ResidualBlock()\n",
    "        # self._res_block_2 = ResidualBlock()\n",
    "        # self._res_block_3 = ResidualBlock()\n",
    "        # self._res_block_4 = ResidualBlock()\n",
    "        # self._dense_1 = Dense(64)\n",
    "        # self._dense_2 = Dense(64)\n",
    "        # self._dense_3 = Dense(32)\n",
    "\n",
    "        # Output conv\n",
    "        self._output_conv = Conv2D(filters= 2,    # QPSK\n",
    "                                   kernel_size=[3,3],\n",
    "                                   padding='same',\n",
    "                                   activation=None)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Input conv\n",
    "        z = self._input_conv(inputs)\n",
    "        # Residual blocks\n",
    "        for i in range(self._num_resblock):\n",
    "            z = self._res_block[i](z)\n",
    "        # z = self._res_block_1(z)\n",
    "        # z = self._res_block_2(z)\n",
    "        # z = self._res_block_3(z)\n",
    "        # z = self._res_block_4(z)\n",
    "        # z = self._dense_1(z)\n",
    "        # z = relu(z)\n",
    "        # z = self._dense_2(z)\n",
    "        # z = relu(z)\n",
    "        # z = self._dense_3(z)\n",
    "        # z = relu(z)\n",
    "        # Output conv\n",
    "        z = self._output_conv(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SUpygto50gIo",
   "metadata": {
    "id": "SUpygto50gIo"
   },
   "source": [
    "#**Load Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Sa9mIc4aDdHH",
   "metadata": {
    "id": "Sa9mIc4aDdHH"
   },
   "source": [
    "- Function to load and preprocess data (y - receiver data and c - label data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "IbHylYKrKfwQ",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1742286640385,
     "user": {
      "displayName": "Nam Khánh",
      "userId": "17195362204309494329"
     },
     "user_tz": -420
    },
    "id": "IbHylYKrKfwQ"
   },
   "outputs": [],
   "source": [
    "def load_hdf5(parent_name, group_name):\n",
    "    with h5py.File(f'{parent_name}.hdf5', \"r\") as f:\n",
    "        b = f[f\"{group_name}_b\"][:]\n",
    "        c = f[f\"{group_name}_c\"][:]\n",
    "        y = f[f\"{group_name}_y\"][:]\n",
    "        r = f[f\"{group_name}_r\"][:]\n",
    "    return b, c, y, r\n",
    "\n",
    "def load_pickle(parent_name, group_name):\n",
    "    \"\"\"Saves data to a pickle file.\"\"\"\n",
    "    def load_from_pickle(filename):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    b = load_from_pickle(f'{parent_name}/{group_name}.b.pkl')\n",
    "    c = load_from_pickle(f'{parent_name}/{group_name}.c.pkl')\n",
    "    y = load_from_pickle(f'{parent_name}/{group_name}.y.pkl')\n",
    "    r = load_from_pickle(f'{parent_name}/{group_name}.r.pkl')\n",
    "\n",
    "    return b, c, y, r\n",
    "\n",
    "def data_loader(df, dir, saved_dataset='hdf5'):\n",
    "    assert saved_dataset in ['hdf5', 'pickle'], \"saved data set should be 'pickle' or 'hdf5'.\"\n",
    "    assert df['nTBSize'].nunique() == 1, \"Not all elements have the same TB size.\"\n",
    "    # assert df['Dmrs_mask'].map(len).nunique() == 1, \"Not all elements have the same number of Dmrs/Data symbols.\"\n",
    "\n",
    "    for pusch_record in df.itertuples():\n",
    "        data_filename = pusch_record.Data_filename\n",
    "        data_dirname = pusch_record.Data_dirname\n",
    "        esno_db = pusch_record.Esno_db\n",
    "        index = pusch_record.index\n",
    "        if saved_dataset == 'hdf5':\n",
    "            b,c,y, r = load_hdf5(f'{dir}/{data_dirname}', data_filename)\n",
    "        else:\n",
    "            b,c,y, r = load_pickle(f'{dir}/{data_dirname}', data_filename)\n",
    "        yield index, esno_db, c, y, b, r\n",
    "\n",
    "def preprocessing(index, esno_db, c, y, b, r):\n",
    "    y = tf.concat([tf.math.real(y), tf.math.imag(y)], axis = 0)\n",
    "    y = tf.transpose(y, perm=[2,1,0])\n",
    "    y = (y - tf.reduce_mean(y)) / tf.math.reduce_std(y)\n",
    "    r = tf.concat([tf.math.real(r), tf.math.imag(r)], axis = 0)\n",
    "    r = tf.transpose(r, perm=[2,1,0])\n",
    "\n",
    "    y_r = tf.concat([y, r], axis = -1)\n",
    "    return index, esno_db, c, y, b, r, y_r\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Dge6oPz9Dgfh",
   "metadata": {
    "id": "Dge6oPz9Dgfh"
   },
   "source": [
    "**Load Data into Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vMtoYhuDMJcj",
   "metadata": {
    "id": "vMtoYhuDMJcj"
   },
   "source": [
    "* **Setup dataset_dir**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ffGbAwJL95D",
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1742286664824,
     "user": {
      "displayName": "Nam Khánh",
      "userId": "17195362204309494329"
     },
     "user_tz": -420
    },
    "id": "1ffGbAwJL95D"
   },
   "outputs": [],
   "source": [
    "dataset_dir = f'/content/'\n",
    "pickles_dir = f'{dataset_dir}/pickle'\n",
    "hdf5_dir = f'{dataset_dir}/hdf5'\n",
    "parquet_dir = f'{dataset_dir}/parquet'\n",
    "\n",
    "parquet_name = '4RB_dataset_186k_samples' #4RB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ELB_xfsMUAQ",
   "metadata": {
    "id": "2ELB_xfsMUAQ"
   },
   "source": [
    "* **Read and split data_folder into train and test set**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8ae75",
   "metadata": {},
   "source": [
    "* Read and load data into DataFrame, then split them into 3 group (SNR -low,mid,high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "X1B5XiP-1ra9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1742286667515,
     "user": {
      "displayName": "Nam Khánh",
      "userId": "17195362204309494329"
     },
     "user_tz": -420
    },
    "id": "X1B5XiP-1ra9",
    "outputId": "1c867bf5-e5d3-450e-bb13-ac3b429da886"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/compat/_optional.py:135\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     module = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/importlib/__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1324\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pyarrow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mparquet_dir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mparquet_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.parquet\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m df = df.reset_index()\n\u001b[32m      4\u001b[39m split_rate = \u001b[32m0.95\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/parquet.py:651\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    498\u001b[39m \u001b[38;5;129m@doc\u001b[39m(storage_options=_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mstorage_options\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_parquet\u001b[39m(\n\u001b[32m    500\u001b[39m     path: FilePath | ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    508\u001b[39m     **kwargs,\n\u001b[32m    509\u001b[39m ) -> DataFrame:\n\u001b[32m    510\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    511\u001b[39m \u001b[33;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[32m    512\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    648\u001b[39m \u001b[33;03m    1    4    9\u001b[39;00m\n\u001b[32m    649\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     impl = \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default:\n\u001b[32m    654\u001b[39m         msg = (\n\u001b[32m    655\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe argument \u001b[39m\u001b[33m'\u001b[39m\u001b[33muse_nullable_dtypes\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will be removed \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    656\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min a future version.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    657\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/parquet.py:78\u001b[39m, in \u001b[36mget_engine\u001b[39m\u001b[34m(engine)\u001b[39m\n\u001b[32m     67\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     68\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to find a usable engine; \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     69\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtried using: \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     74\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m     )\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPyArrowImpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine == \u001b[33m\"\u001b[39m\u001b[33mfastparquet\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m FastParquetImpl()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/io/parquet.py:163\u001b[39m, in \u001b[36mPyArrowImpl.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpyarrow is required for parquet support.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    165\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyarrow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mparquet\u001b[39;00m\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# import utils to register the pyarrow extension types\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/pandas/compat/_optional.py:138\u001b[39m, in \u001b[36mimport_optional_dependency\u001b[39m\u001b[34m(name, extra, errors, min_version)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m    137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors == \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow."
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(f'{parquet_dir}/{parquet_name}.parquet', engine=\"pyarrow\")\n",
    "df = df.reset_index()\n",
    "\n",
    "split_rate = 0.95\n",
    "\n",
    "df_low = df[df['Esno_db'] <= -5]\n",
    "df_mid = df[(df['Esno_db'] <= 0) & (df['Esno_db'] > -5)]\n",
    "df_high = df[df['Esno_db'] > -0]\n",
    "\n",
    "NUM_SAMPLE_LOW = len(df_low)\n",
    "NUM_SAMPLE_MID = len(df_mid)\n",
    "NUM_SAMPLE_HIGH = len(df_high)\n",
    "\n",
    "df_low = df_low.sample(frac=1)\n",
    "df_mid = df_mid.sample(frac=1)\n",
    "df_high = df_high.sample(frac=1)\n",
    "\n",
    "test_df_low = df_low.iloc[int(NUM_SAMPLE_LOW*split_rate):]\n",
    "train_df_low = df_low.iloc[:int(NUM_SAMPLE_LOW*split_rate)]\n",
    "\n",
    "test_df_mid = df_mid.iloc[int(NUM_SAMPLE_MID*split_rate):]\n",
    "train_df_mid = df_mid.iloc[:int(NUM_SAMPLE_MID*split_rate)]\n",
    "\n",
    "test_df_high = df_high.iloc[int(NUM_SAMPLE_HIGH*split_rate):]\n",
    "train_df_high = df_high.iloc[:int(NUM_SAMPLE_HIGH*split_rate)]\n",
    "\n",
    "train_df = pd.concat([train_df_low, train_df_mid, train_df_high]).reset_index(drop=True).reset_index()\n",
    "\n",
    "print(f\"Low: Total Size: {len(df_low)} sample, Test Size: {len(test_df_low)} sample, Train Size: {len(train_df_low)} sample\")\n",
    "print(f\"Mid: Total Size: {len(df_mid)} sample, Test Size: {len(test_df_mid)} sample, Train Size: {len(train_df_mid)} sample\")\n",
    "print(f\"High: Total Size: {len(df_high)} sample, Test Size: {len(test_df_high)} sample, Train Size: {len(train_df_high)} sample\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dWtlXpeANKG9",
   "metadata": {
    "id": "dWtlXpeANKG9"
   },
   "source": [
    "* **Load data into Dataset object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "YVRlHskV_-9P",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1742286668884,
     "user": {
      "displayName": "Nam Khánh",
      "userId": "17195362204309494329"
     },
     "user_tz": -420
    },
    "id": "YVRlHskV_-9P"
   },
   "outputs": [],
   "source": [
    "def load_data(train_df, test_df, hdf5_dir, batch_train, batch_test):\n",
    "  train_set = tf.data.Dataset.from_generator(\n",
    "            lambda: data_loader(train_df, hdf5_dir),\n",
    "            output_types=(tf.int32, tf.float32, tf.float32, tf.complex64, tf.float32, tf.complex64))\n",
    "\n",
    "  train_set = train_set.cache()\n",
    "  train_set = train_set.prefetch(tf.data.AUTOTUNE)\n",
    "  train_set = train_set.map(preprocessing).batch(batch_train)\n",
    "\n",
    "  # Load test_data into Dataset\n",
    "  test_set = tf.data.Dataset.from_generator(\n",
    "              lambda: data_loader(test_df, hdf5_dir),\n",
    "              output_types=(tf.int32, tf.float32, tf.float32, tf.complex64, tf.float32, tf.complex64))\n",
    "\n",
    "  test_set = test_set.cache()\n",
    "  test_set = test_set.prefetch(tf.data.AUTOTUNE)\n",
    "  test_set = test_set.map(preprocessing).batch(batch_test)\n",
    "  return train_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "AEAdU7XEGLsp",
   "metadata": {
    "executionInfo": {
     "elapsed": 550,
     "status": "ok",
     "timestamp": 1742286672292,
     "user": {
      "displayName": "Nam Khánh",
      "userId": "17195362204309494329"
     },
     "user_tz": -420
    },
    "id": "AEAdU7XEGLsp"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m BATCH_TRAIN = \u001b[32m256\u001b[39m\n\u001b[32m      2\u001b[39m BATCH_TEST = \u001b[32m64\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m train_set, test_set_low = load_data(\u001b[43mtrain_df\u001b[49m, test_df_low, hdf5_dir, BATCH_TRAIN, BATCH_TEST)\n\u001b[32m      5\u001b[39m _, test_set_mid = load_data(train_df_mid, test_df_mid, hdf5_dir, BATCH_TRAIN, BATCH_TEST)\n\u001b[32m      6\u001b[39m _, test_set_high = load_data(train_df_high, test_df_high, hdf5_dir, BATCH_TRAIN, BATCH_TEST)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_TRAIN = 256\n",
    "BATCH_TEST = 64\n",
    "\n",
    "train_set, test_set_low = load_data(train_df, test_df_low, hdf5_dir, BATCH_TRAIN, BATCH_TEST)\n",
    "_, test_set_mid = load_data(train_df_mid, test_df_mid, hdf5_dir, BATCH_TRAIN, BATCH_TEST)\n",
    "_, test_set_high = load_data(train_df_high, test_df_high, hdf5_dir, BATCH_TRAIN, BATCH_TEST)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "gXh2J0zEN__s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145089,
     "status": "ok",
     "timestamp": 1742286824641,
     "user": {
      "displayName": "Nam Khánh",
      "userId": "17195362204309494329"
     },
     "user_tz": -420
    },
    "id": "gXh2J0zEN__s",
    "outputId": "4c4fe93e-910f-439d-f775-3d926bdc2d89"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_set_low' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# First load data for accelerating\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n, (index, esno_db, c, y, b, r, y_r) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mtest_set_low\u001b[49m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(y.shape)\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n, (index, esno_db, c, y, b, r, y_r) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_set_mid):\n",
      "\u001b[31mNameError\u001b[39m: name 'test_set_low' is not defined"
     ]
    }
   ],
   "source": [
    "# First load data for accelerating\n",
    "for n, (index, esno_db, c, y, b, r, y_r) in enumerate(test_set_low):\n",
    "    print(y.shape)\n",
    "\n",
    "for n, (index, esno_db, c, y, b, r, y_r) in enumerate(test_set_mid):\n",
    "    print(y.shape)\n",
    "\n",
    "for n, (index, esno_db, c, y, b, r, y_r) in enumerate(test_set_high):\n",
    "    print(y.shape)\n",
    "\n",
    "for n, (index, esno_db, c, y, b, r, y_r) in enumerate(train_set):\n",
    "    print(y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zqq2MzTGo2JP",
   "metadata": {
    "id": "zqq2MzTGo2JP"
   },
   "source": [
    "- Checking size of a sample data in Dataset\n",
    "- Preprocessing is called when load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PJTxVc3qXrct",
   "metadata": {
    "id": "PJTxVc3qXrct"
   },
   "source": [
    "#**Training Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3LwiUPJm-9NK",
   "metadata": {
    "id": "3LwiUPJm-9NK"
   },
   "source": [
    "\n",
    "\n",
    "*   Creat Model and Optimizer Instance\n",
    "*   Set mode of \"pretrained\" ( True to load pretrained weights if exist )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "efc3d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ANTENNA_8RX = 18\n",
    "NUM_ANTENNA_4RX = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "GfAq_2mp5Pkc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "executionInfo": {
     "elapsed": 2843,
     "status": "ok",
     "timestamp": 1742286827509,
     "user": {
      "displayName": "Nam Khánh",
      "userId": "17195362204309494329"
     },
     "user_tz": -420
    },
    "id": "GfAq_2mp5Pkc",
    "outputId": "8c9bcbf9-9b11-446b-e4ca-debba65ef355"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"custom_neural_receiver_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"custom_neural_receiver_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ separable_conv2d_56             │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">794</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SeparableConv2D</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_26               │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_27               │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_28               │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_29               │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_30               │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_31               │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_32               │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_33               │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_34               │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_35               │ ?                      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,472</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ResidualBlock</span>)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,154</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ separable_conv2d_56             │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │           \u001b[38;5;34m794\u001b[0m │\n",
       "│ (\u001b[38;5;33mSeparableConv2D\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_26               │ ?                      │         \u001b[38;5;34m9,472\u001b[0m │\n",
       "│ (\u001b[38;5;33mResidualBlock\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_27               │ ?                      │         \u001b[38;5;34m9,472\u001b[0m │\n",
       "│ (\u001b[38;5;33mResidualBlock\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_28               │ ?                      │         \u001b[38;5;34m9,472\u001b[0m │\n",
       "│ (\u001b[38;5;33mResidualBlock\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_29               │ ?                      │         \u001b[38;5;34m9,472\u001b[0m │\n",
       "│ (\u001b[38;5;33mResidualBlock\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_30               │ ?                      │         \u001b[38;5;34m9,472\u001b[0m │\n",
       "│ (\u001b[38;5;33mResidualBlock\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_31               │ ?                      │         \u001b[38;5;34m9,472\u001b[0m │\n",
       "│ (\u001b[38;5;33mResidualBlock\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_32               │ ?                      │         \u001b[38;5;34m9,472\u001b[0m │\n",
       "│ (\u001b[38;5;33mResidualBlock\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_33               │ ?                      │         \u001b[38;5;34m9,472\u001b[0m │\n",
       "│ (\u001b[38;5;33mResidualBlock\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_34               │ ?                      │         \u001b[38;5;34m9,472\u001b[0m │\n",
       "│ (\u001b[38;5;33mResidualBlock\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ residual_block_35               │ ?                      │         \u001b[38;5;34m9,472\u001b[0m │\n",
       "│ (\u001b[38;5;33mResidualBlock\u001b[0m)                 │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m2\u001b[0m)         │         \u001b[38;5;34m1,154\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">96,668</span> (377.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m96,668\u001b[0m (377.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">96,668</span> (377.61 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m96,668\u001b[0m (377.61 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CustomNeuralReceiver(10)\n",
    "inputs = tf.zeros([1,48,14,NUM_ANTENNA_4RX])\n",
    "model(inputs)\n",
    "model.summary()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "pretrained = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4ea513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e32a3421",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model = ov.convert_model(model, input=[-1,48,14,10])\n",
    "ov.save_model(ov_model, 'VHT_96k_model.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "628e8dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = Core()\n",
    "\n",
    "# Load model\n",
    "model_path = \"new_model.xml\"  # hoặc .onnx, .bin,...\n",
    "model = core.read_model(model=model_path)\n",
    "model.reshape([2,48,14,10])\n",
    "ov.save_model(model, 'new_model_1.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "6494dd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.57076644897461\n",
      "Output shape: (40, 48, 14, 2)\n"
     ]
    }
   ],
   "source": [
    "from openvino.runtime import Core\n",
    "import numpy as np\n",
    "import time\n",
    "# Khởi tạo OpenVINO runtime\n",
    "core = Core()\n",
    "\n",
    "# Load model\n",
    "model_path = \"new_model.xml\"  # hoặc .onnx, .bin,...\n",
    "model = core.read_model(model=model_path)\n",
    "\n",
    "# Compile model\n",
    "compiled_model = core.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "# Tạo 1 Infer Request\n",
    "infer_request = compiled_model.create_infer_request()\n",
    "\n",
    "# Chuẩn bị input\n",
    "input_layer = compiled_model.input(0)\n",
    "dummy_input = np.random.randn(40, 48, 14, 10).astype(np.float32)\n",
    "infer_request.infer({input_layer: dummy_input})\n",
    "dummy_input = np.random.randn(40, 48, 14, 10).astype(np.float32)\n",
    "start = time.time()\n",
    "# Gửi request infer\n",
    "infer_request.infer({input_layer: dummy_input})\n",
    "\n",
    "# Lấy output\n",
    "output_layer = compiled_model.output(0)\n",
    "result = infer_request.get_output_tensor(output_layer.index).data\n",
    "print((time.time()-start)*1000)\n",
    "print(\"Output shape:\", result.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_F1sbTBgA9lA",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1742286827513,
     "user": {
      "displayName": "Nam Khánh",
      "userId": "17195362204309494329"
     },
     "user_tz": -420
    },
    "id": "_F1sbTBgA9lA"
   },
   "outputs": [],
   "source": [
    "# Define our metrics\n",
    "train_batch_loss = tf.keras.metrics.Mean('Epoch_train_loss', dtype=tf.float32)\n",
    "val_batch_loss_low = tf.keras.metrics.Mean('Epoch_val_loss_low', dtype=tf.float32)\n",
    "val_batch_loss_mid = tf.keras.metrics.Mean('Epoch_val_loss_mid', dtype=tf.float32)\n",
    "val_batch_loss_high = tf.keras.metrics.Mean('Epoch_val_loss_high', dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s5D-1p8QpyWI",
   "metadata": {
    "id": "s5D-1p8QpyWI"
   },
   "source": [
    "- **Define Train_step and Loss_cal**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9WHkZ-9d555E",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1742286827516,
     "user": {
      "displayName": "Nam Khánh",
      "userId": "17195362204309494329"
     },
     "user_tz": -420
    },
    "id": "9WHkZ-9d555E"
   },
   "outputs": [],
   "source": [
    "def loss_cal(pred, labels):\n",
    "  bce = tf.nn.sigmoid_cross_entropy_with_logits(labels, pred)\n",
    "  bce = tf.reduce_mean(bce)\n",
    "  loss = bce\n",
    "  return loss\n",
    "\n",
    "\n",
    "#train model with nRB\n",
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    llr = model(inputs)\n",
    "\n",
    "    # Convert output size to label size (Remove DMRS, Demapper)\n",
    "    llr = tf.concat([llr[...,0:3,:],llr[...,4:11,:], llr[...,12:14,:]],axis=-2)\n",
    "    llr = tf.transpose(llr, perm=[0,2,1,3])\n",
    "    llr = tf.reshape(llr, [llr.shape[0],(llr.shape[1]*llr.shape[2]*llr.shape[3])])\n",
    "\n",
    "    # Loss calculation\n",
    "    loss = loss_cal(llr, labels)\n",
    "\n",
    "  # Apply Gradient Descent\n",
    "  weights = model.trainable_weights\n",
    "  grads = tape.gradient(loss, weights)\n",
    "  optimizer.apply_gradients(zip(grads, weights))\n",
    "\n",
    "  return loss\n",
    "\n",
    "#test model with nRB\n",
    "@tf.function\n",
    "def val_step(inputs, labels):\n",
    "  llr = model(inputs)\n",
    "  llr = tf.concat([llr[...,0:3,:],llr[...,4:11,:], llr[...,12:14,:]],axis=-2)\n",
    "  llr = tf.transpose(llr, perm=[0,2,1,3])\n",
    "  llr = tf.reshape(llr, [llr.shape[0],(llr.shape[1]*llr.shape[2]*llr.shape[3])])\n",
    "  loss = loss_cal(llr, labels)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lshclj0Ks5Pa",
   "metadata": {
    "id": "lshclj0Ks5Pa"
   },
   "source": [
    "- **Load and save pretrained Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8M5r4N05AkAn",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1742286827520,
     "user": {
      "displayName": "Nam Khánh",
      "userId": "17195362204309494329"
     },
     "user_tz": -420
    },
    "id": "8M5r4N05AkAn"
   },
   "outputs": [],
   "source": [
    "def load_weights(model, pretrained_weights_path):\n",
    "    # Build Model with random input\n",
    "    # Load weights\n",
    "  with open(pretrained_weights_path, 'rb') as f:\n",
    "    weights = pickle.load(f)\n",
    "    model.set_weights(weights)\n",
    "    print(f\"Loaded pretrained weights from {pretrained_weights_path}\")\n",
    "\n",
    "def save_weights(model, model_folder_path, model_file_name):\n",
    "    # Save the weights in a file\n",
    "    model_weights_path = os.path.join(model_folder_path, model_file_name)\n",
    "    weights = model.get_weights()\n",
    "    with open(model_weights_path, 'wb') as f:\n",
    "        pickle.dump(weights, f)\n",
    "\n",
    "def save_trainable_weights(model, model_folder_path, model_file_name):\n",
    "    # Save the weights in a file\n",
    "    model_weights_path = os.path.join(model_folder_path, model_file_name)\n",
    "    weights = model.trainable_weights\n",
    "    with open(model_weights_path, 'wb') as f:\n",
    "        pickle.dump(weights, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abaee13",
   "metadata": {},
   "source": [
    "* Load pretrained weight if existed ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O5r3NrSIse5z",
   "metadata": {
    "id": "O5r3NrSIse5z"
   },
   "outputs": [],
   "source": [
    "if pretrained:\n",
    "  pretrained_weights_path = '/content/drive/MyDrive/AI_for_PUSCH/VHT_neural_receiver/weight_4RB_normalize_with_DMRS_org.pkl'\n",
    "  load_weights(model, pretrained_weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NCwCmdyQRbf-",
   "metadata": {
    "id": "NCwCmdyQRbf-"
   },
   "source": [
    "- **Training model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NN8aU0OIEVOm",
   "metadata": {
    "id": "NN8aU0OIEVOm"
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GX1JB3fpj9PB",
   "metadata": {
    "executionInfo": {
     "elapsed": 725,
     "status": "ok",
     "timestamp": 1742286828248,
     "user": {
      "displayName": "Nam Khánh",
      "userId": "17195362204309494329"
     },
     "user_tz": -420
    },
    "id": "GX1JB3fpj9PB"
   },
   "outputs": [],
   "source": [
    "#Initialize folder log for tensorboard\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "#Setup log_path\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "\n",
    "val_log_dir = 'logs/gradient_tape/' + current_time + '/val'\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LZWckpuojhTE",
   "metadata": {
    "id": "LZWckpuojhTE"
   },
   "source": [
    "* ***If using VSCode, run \"tensorboard --logdir /log_path\" in terminal***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tx2RaLbhEn5W",
   "metadata": {
    "id": "tx2RaLbhEn5W"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/gradient_tape/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tYvC5M2OTgGu",
   "metadata": {
    "id": "tYvC5M2OTgGu"
   },
   "source": [
    "\n",
    "\n",
    "*   **Define global params for training**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pU1wnUKbTXW0",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1742286828256,
     "user": {
      "displayName": "Nam Khánh",
      "userId": "17195362204309494329"
     },
     "user_tz": -420
    },
    "id": "pU1wnUKbTXW0"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# Define num EPOCHS, Model_path, Model_name to save model during training\n",
    "EPOCHS = 1000\n",
    "SAVE_AFTER_NUM_EPOCH = 2\n",
    "model_folder_path = \"VHT_neural_receiver/\"    #Custom here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9nR6d7b_zwW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9nR6d7b_zwW",
    "outputId": "cbd322b1-6b75-4408-8e5e-070dbf01da24"
   },
   "outputs": [],
   "source": [
    "if not pretrained:\n",
    "  pre_batch_loss = 1e9\n",
    "  for epoch in range(EPOCHS):\n",
    "      start_time = time.time()\n",
    "      batch_loss = np.array([])\n",
    "      batch_val_loss_low = np.array([])\n",
    "      batch_val_loss_mid = np.array([])\n",
    "      batch_val_loss_high = np.array([])\n",
    "      print(\n",
    "        f'Epoch {epoch}, '\n",
    "      )\n",
    "\n",
    "    # Load batch data for 4RB training\n",
    "      for iter, (index, esno_db, c, y, b,r, y_r) in enumerate(train_set):\n",
    "          # calculate loss and optimize\n",
    "          loss = train_step(y_r, c)\n",
    "          batch_loss = np.append(batch_loss, loss)\n",
    "\n",
    "\n",
    "      # Load batch data for 4RB validating\n",
    "      for iter, (index, esno_db, c, y, b,r, y_r) in enumerate(test_set_low):\n",
    "          loss_val = val_step(y_r, c)\n",
    "          batch_val_loss_low = np.append(batch_val_loss_low, loss_val)\n",
    "\n",
    "      for iter, (index, esno_db, c, y, b,r, y_r) in enumerate(test_set_mid):\n",
    "          loss_val = val_step(y_r, c)\n",
    "          batch_val_loss_mid = np.append(batch_val_loss_mid, loss_val)\n",
    "\n",
    "      for iter, (index, esno_db, c, y, b,r, y_r) in enumerate(test_set_high):\n",
    "          loss_val = val_step(y_r, c)\n",
    "          batch_val_loss_high = np.append(batch_val_loss_high, loss_val)\n",
    "\n",
    "\n",
    "      # Write batch loss to log file\n",
    "      train_batch_loss(tf.reduce_mean(batch_loss))\n",
    "      with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('BCE_Epoch_Loss', train_batch_loss.result(), step=epoch+134)\n",
    "\n",
    "      val_batch_loss_low(tf.reduce_mean(batch_val_loss_low))\n",
    "      with val_summary_writer.as_default():\n",
    "        tf.summary.scalar('BCE_Epoch_Loss', val_batch_loss_low.result(), step=epoch+134)\n",
    "\n",
    "      val_batch_loss_mid(tf.reduce_mean(batch_val_loss_mid))\n",
    "      with val_summary_writer.as_default():\n",
    "        tf.summary.scalar('BCE_Epoch_Loss', val_batch_loss_mid.result(), step=epoch+134)\n",
    "\n",
    "      val_batch_loss_high(tf.reduce_mean(batch_val_loss_high))\n",
    "      with val_summary_writer.as_default():\n",
    "        tf.summary.scalar('BCE_Epoch_Loss', val_batch_loss_high.result(), step=epoch+134)\n",
    "\n",
    "      time_taken = time.time() - start_time\n",
    "\n",
    "      # Log print\n",
    "      print(\n",
    "          f'Epoch {epoch}, '\n",
    "          f'Train_Loss: {train_batch_loss.result():0.4f}, '\n",
    "          f'Val_Loss_low: {val_batch_loss_low.result():0.4f}, '\n",
    "          f'Val_Loss_mid: {val_batch_loss_mid.result():0.4f}, '\n",
    "          f'Val_Loss_high: {val_batch_loss_high.result():0.4f}, '\n",
    "          f'Time taken: {time_taken:0.2f}'\n",
    "      )\n",
    "      \n",
    "      # Update weight during training if (high/mid/low) val decrease (Could cmt)\n",
    "      # if pre_batch_loss >= val_batch_loss_high.result():\n",
    "      #   pre_batch_loss = val_batch_loss_high.result()\n",
    "      #   model_file_name = f\"weight_4RB_high_SNR_dynamic_config.pkl\"\n",
    "      #   save_weights(model, model_folder_path, model_file_name)\n",
    "      #   print(\n",
    "      #       f'Save model at epoch {epoch}'\n",
    "      #   )\n",
    "\n",
    "      train_batch_loss.reset_state()\n",
    "      val_batch_loss_low.reset_state()\n",
    "      val_batch_loss_mid.reset_state()\n",
    "      val_batch_loss_high.reset_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wzgzsX_Zs9-3",
   "metadata": {
    "id": "wzgzsX_Zs9-3"
   },
   "source": [
    "* **Save weight**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scbnGQGKqQRO",
   "metadata": {
    "id": "scbnGQGKqQRO"
   },
   "outputs": [],
   "source": [
    "    # Save the weights in a file\n",
    "model_folder_path = \"VHT_neural_receiver/\"\n",
    "model_file_name = \"weight_name.pkl\"\n",
    "save_weights(model, model_folder_path, model_file_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1L4UGLYGtYC7z7Gsmsx3fY7RZ3yboLa-y",
     "timestamp": 1739113707037
    },
    {
     "file_id": "https://github.com/nvlabs/sionna/blob/main/examples/Neural_Receiver.ipynb",
     "timestamp": 1737450903303
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
